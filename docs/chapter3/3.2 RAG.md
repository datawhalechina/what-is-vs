# Retrieval-Augmented Generation (RAG) 检索增强的生成模型

在人工智能的众多分支中，自然语言处理（NLP）无疑是最具挑战性的领域之一。随着技术的进步，我们已经从简单的文本匹配转向了能够理解和生成复杂语言的智能系统。在这一进程中，"Retrieval-Augmented Generation"（RAG），或称增强检索的生成模型，成为了近年来的一个重要创新。本文将全面介绍RAG的技术细节、应用场景以及它对未来NLP应用的可能影响。下文中 RAG、RAG 模型和检索增强的生成模型均为同一意思。

## 什么是RAG？

RAG模型结合了传统的文本生成技术和现代的信息检索系统，旨在提升生成文本的准确性和相关性。这种模型通过先从一个庞大的文档数据库中检索出与输入相关的文档，再利用这些信息来指导文本的生成过程。这样的设计允许模型不仅仅依赖于其内部的语言模型和训练数据，而是能够访问外部的详细信息，从而生成更为丰富和精准的回答。

检索增强的生成模型这个概念随着以 ChatGPT 或者 GPT-4 为代表的 Large Language Models (LLMs) 大语言模型的火热而逐渐也在学术圈成为常见话题。RAG 结构确实是一定程度上能解决大语言模型诸多问题的方法之一，但是这个概念却并不是起源于大语言模型。没有引入大型语言模型的检索增强的生成模型模型仍然保持着基本的检索增强生成框架，但其核心生成器部分不依赖于复杂的预训练语言模型如早期的 GPT-3 等。在这种情况下，RAG 的生成器可能是一个更简单或更传统的机器学习模型，用于处理文本生成，而重点是如何利用检索到的信息来指导这一过程。

RAG模型可以分为两大核心组成部分：检索器（Retriever）和生成器（Generator）。

- 检索器：这一部分的任务是从数据源中找到与输入查询最相关的信息。检索器通常基于深度学习的向量空间模型，如BERT或其他变体，这些模型能够将文本转换成高维空间中的向量。然后，通过计算查询向量与数据库中的文档向量之间的相似度，检索出最相关的文档。

- 生成器：一旦相关文档被检索出来，生成器则负责处理这些文档，并生成相应的文本。生成器通常是基于Transformer的模型，如 GPT，它们擅长生成连贯和语境相关的文本。生成器会综合输入的查询和检索到的文档内容，生成一个既准确又信息完备的输出。

RAG 已被应用于多个实际场景，其中包括但不限于：

- 问答系统：RAG 能够提供比传统问答系统更准确、更详细的答案，尤其是在需要引用具体数据或历史事实时。
- 内容推荐：通过分析用户的查询并检索相关内容，RAG 可以在新闻聚合、视频推荐等领域提供个性化的内容推荐。
- 自动摘要：RAG 可以从多个文档中提取关键信息，生成涵盖主要观点的摘要，这对于快速浏览大量信息尤其有用。

在RAG系统中，当一个查询输入到系统时，检索器首先工作，找到最相关的文档或文本片段。这些文档随后被送到生成器，生成器读取这些信息，并结合原始查询来生成回答。这种机制特别适用于那些需要广泛背景知识来回答的问题，如专业领域的问题解答或复杂的常识性问题。

## RAG 和大语言模型 (LLMs)

随着 ChatGPT 等大语言模型（LLM）的流行，我们发现原来聊天机器人也可以这么智能。当我们以为这种机器人可以取代传统搜索引擎的时候，我们发现事实上大语言模型在现实世界应用中表现得并不理想。比如一项重大短板就是幻觉问题，即LLMs产生似是而非但实际上不正确或忠实无意义的信息。这背后有许多潜在原因。首先，LLMs缺乏领域知识。LLMs主要在公共数据集上进行训练，这不可避免地导致了它们回答那些超出内部知识范围的领域特定问题的能力有限。此外，LLMs进行实时知识更新也是一大挑战。即使问题在LLMs的学习语料库范围内，其答案仍可能显示出限制，因为当外部世界动态变化时，内部知识可能已经过时了。最后但同样重要的是，发现LLMs存在偏见。用来训练LLMs的数据集很大，可能引入系统性错误，比如模仿性的造词造句、冗余、和社会偏见在内的每个数据集都可能存在偏见问题。第二，商业上集成LLMs的另一个缺点是成本高昂（Schwartz等，2020）。对于一般的商业实体来说，将LLMs应用于商业用途几乎是不可行的。对于非技术公司来说，定制和训练自己版权的 GPT 模型几乎是不可能的，因为他们没有进行如此大型项目的资源和人才，而且频繁地调用第三方提供商如 OpenAI 的 API 可能极其昂贵，即使是最近推出的 GPT 商店也不能很好的解决企业用途的生成式模型的需求，更不用说在某些地区这类提供商的数量非常有限。第三，LLMs的遗忘问题一直颇具争议，因为发现LLMs倾向于忘记之前输入的信息。研究表明，LLMs也表现出神经网络所具有的灾难性遗忘行为。为此，检索增强生成（RAG）作为一个新颖的解决方案出现，它通过在外部数据库中整合和处理大量动态数据来解决LLM所面临的挑战，VecDBs在此扮演着LLMs的外部记忆库的角色。通过将私有数据分割并转换成向量，VecDBs能够高效存储这些向量，以实现快速检索过程。与VecDBs的集成使得LLMs能够访问和合成大量数据而无需不断重新训练，从而克服了它们的固有局限。

## RAG 和向量数据库 (VecDBs)

在 RAG 的原型中，我们利用这一思想来克服 LLM 的幻觉问题，通过提供具有准确指示的领域特定数据。VecDBs作为外部知识库，存储领域特定数据，使LLM能够轻松处理用户拥有的大量数据。这里向量数据库在数据存储中扮演重要角色，利用向量数据库这种的可靠外部记忆库，可以完成大量且多样的数据收集工作。首先是数据预处理阶段，收集、清理、整合、转换、规茨化和标准化原始数据。由于 LLMs 的上下文限制，处理后的数据被切割成小块。这些数据块随后被嵌入模型转换成向量——数据的语义表示，并存储在向量数据库中，供后续的向量搜索使用。一个发展成熟的向量数据库会妥善索引数据并优化检索流程。许多研究和行业实例证明了 RAG 的深远影响。例如，Azure AI Search（原 Cognitive Search）开发了使用 Qdrant 3 的向量搜索。另一个例子是 Pinecone 的 Canopy 4，一个利用 Pinecone 的向量数据库构建RAG系统的开源框架。Spotify 和 Yahoo 等公司采用了 Vespa 这一 AI 驱动的在线向量数据库，Yahoo 用它来帮助用户直接与他们的邮箱对话，询问邮箱相关的问题，并告诉它执行操作。这种方法将个性化数据与LLMs整合，形成了 RAG 系统。这些例子展示了 RAG 结合 VecDBs 的力量，不仅可以帮助企业在管理和提取多样化数据集方面面临的独特挑战，还展示了如何通过与VecDBs的集成解决LLMs面临的前述困难。
